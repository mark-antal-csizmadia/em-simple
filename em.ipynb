{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Parameters with the Expectation Maximization (EM) Algorithm for Partially Observed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, a simple EM algorithm is used to learn the parameters of a three node, v-structured Bayesian network such that (X -> Z <- Y) for partially observed data. \n",
    "\n",
    "All of the variables are drawn from a binomial distribution, that is, they are binary, {0, 1}. Each variable has a CPD table of type numpy.ndarray representing the probability distribtuion parameters - the parameters theta in the pseudo-code below.\n",
    "\n",
    "X and Y has no parents, therefore their CPD has only two parameter estimates for P(Var=1) and P(Var=0). Z has eight parameters since it has two parents. Its CPD is a 3D numpy.ndarray such that pz=[[[p(z=0|x=0, y=0) p(z=1|x=0, y=0)], [...]], [[...], [...]]] where the indexing is such that p(z|x, y) is at pz[x, y, z]. As a result, the marginals p[x, y, :] sum to unity since p(z=0|x, y) + p(z=1|x, y) = 1. Furhermore, marginals over x and y can also be computed from the CPD of Z.\n",
    "\n",
    "For the EM to work with missing data, the data observability model has to be Missing At Random (MAR). This conecept, and teh concept of sufficient statistics of distribtuions (here binomial) are in the centre of focus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo-code of EM for the v-structured Bayesian network\n",
    "\n",
    "<pre>\n",
    "<i><b>Expectation-Maximization</b></i> (\n",
    "    &theta;<sup>(0)</sup> // Initial parameters\n",
    "    D   // Data\n",
    "  )\n",
    "1    <b>for</b> each iteration t\n",
    "2        M<sub>t</sub> &larr; E-step(&theta;<sup>(t)</sup>, D)\n",
    "3        &theta;<sup>(t+1)</sup> &larr; M-step(M<sub>t</sub>)\n",
    "4    <b>return</b> &theta;<sup>(T)</sup>\n",
    "\n",
    "<i><b>E-step</b></i> (\n",
    "    &theta;<sup>(t)</sup> // Current parameters\n",
    "    D   // Data\n",
    "  )\n",
    "1    M<sub>t</sub> &larr; 0\n",
    "2    <b>for</b> each data point d\n",
    "3        <b>for</b> each node X\n",
    "4            <b>for</b> each combination X=x, Pa(X)=y\n",
    "5                M<sub>t</sub>[x, y] &larr; M<sub>t</sub>[x, y] + p(x, y|d, &theta;<sup>(t)</sup>)\n",
    "6    <b>return</b> M<sub>t</sub>\n",
    "    \n",
    "\n",
    "<i><b>M-step</b></i> (\n",
    "    M<sub>t</sub>  // Expected sufficient statistics\n",
    "  )\n",
    "1    <b>for</b> each node X\n",
    "2        <b>for</b> each combination X=x, Pa(X)=y\n",
    "3            &theta;<sup>(t+1)</sup>[x|y] &larr; M<sub>t</sub>[x, y] / M<sub>t</sub>[p]\n",
    "4    <b>return</b> &#x03B8;<sup>(t+1)</sup>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectation_maximization(x, y, z, e_step, m_step, num_iter):\n",
    "    \"\"\" Performs Expectation-Maximization algorithm and checks correctness.\n",
    "    It only works for a V-structure, i.e. X -> Z <- Y\n",
    "    It does not allow missing z, but both or either of x and y can be missing.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y, z : numpy.ndarray\n",
    "        Input data where a None in x or y is interpreted as missing data. \n",
    "    e_step : function\n",
    "        A function that takes current parameter estimates qx, qy, qz and data x, y, z \n",
    "        and outputs expected sufficient statistics Mx, My, Mz.\n",
    "    m_step :function\n",
    "        A function that takes current expected sufficient statistics \n",
    "        and outputs new parameter estimates qx, qy, qz.\n",
    "    num_iter : int\n",
    "        The number of iterations to run EM.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    qx, qy, qx : numpy.ndarray\n",
    "        Final parameter estimates after num_iter iterations of e_step and m_step.\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    qx, qy, qz = initialize_parameters()\n",
    "    for i in range(num_iter):\n",
    "        Mx, My, Mz = e_step(qx, qy, qz, x, y, z)\n",
    "        # Assert valid statistics\n",
    "        assert np.isclose(np.sum(Mx), n), f\"Mx = {Mx} should sum to {n}, but is {np.sum(Mx)}\"\n",
    "        assert np.isclose(np.sum(My), n), f\"My = {My} should sum to {n}, but is {np.sum(My)}\"\n",
    "        assert np.isclose(np.sum(Mz[0]), Mx[0]), \\\n",
    "            f\"Mz[0] = {Mz[0]} should sum to Mx[0] = {Mx[0]}, but is {np.sum(Mz[0])}\"\n",
    "        assert np.isclose(np.sum(Mz[1]), Mx[1]), \\\n",
    "            f\"Mz[1] = {Mz[1]} should sum to Mx[1] = {Mx[1]}, but is {np.sum(Mz[1])}\"\n",
    "        assert np.isclose(np.sum(Mz[:, 0]), My[0]), \\\n",
    "            f\"Mz[:, 0] = {Mz[:, 0]} should sum to My[0] = {My[0]}, but is {np.sum(Mz[:, 0])}\"\n",
    "        assert np.isclose(np.sum(Mz[:, 1]), My[1]), \\\n",
    "            f\"Mz[:, 1] = {Mz[:, 1]} should sum to My[1] = {My[1]}, but is {np.sum(Mz[:, 1])}\"\n",
    "\n",
    "        qx, qy, qz = m_step(Mx, My, Mz)\n",
    "        # Assert valid parameters\n",
    "        assert (qx >= 0).all(), f\"qx = {qx} need to be non-negative\"\n",
    "        assert (qy >= 0).all(), f\"qy = {qy} need to be non-negative\"\n",
    "        assert (qz >= 0).all(), f\"qz = {qz} need to be non-negative\"\n",
    "        assert np.isclose(np.sum(qx), 1), f\"qx = {qx} need to sum to one\"\n",
    "        assert np.isclose(np.sum(qy), 1), f\"qy = {qy} need to sum to one\"\n",
    "        assert np.isclose(np.sum(qz, axis=2), 1).all(), f\"Each row of qz = {qz} needs to sum to one\"\n",
    "    return qx, qy, qz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(px, py, pz, n, *, partially_observed=False, never_coobserved=False):\n",
    "    \"\"\" Generates data given table-CPDs of a V-stucture X -> Z <- Y\n",
    "    It can generate complete or partially observed data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    px, py, px : numpy.ndarray\n",
    "        Parameters to generate data with.\n",
    "    n : int\n",
    "        Number of data points.\n",
    "    partially_observed : bool\n",
    "        If True, half of x and y will be missing (set to None)\n",
    "    never_coobserved : bool\n",
    "        If True, y is missing if and only if x is observed, so that no data points contains both x and y. \n",
    "        If False, y is missing independently of whether x is missing. \n",
    "        Has no effect if partially_observed is False.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x, y, z : numpy.ndarray\n",
    "        Generated data where a None in x or y is interpreted as missing data. \n",
    "    \"\"\"\n",
    "    assert px.shape == (2,), f\"px = {px} should have shape (2,)\"\n",
    "    assert py.shape == (2,), f\"py = {py} should have shape (2,)\"\n",
    "    assert pz.shape == (2, 2, 2), f\"pz = {pz} should have shape (2, 2, 2)\"\n",
    "    x = np.argmax(np.random.multinomial(1, px, n), axis=1)\n",
    "    y = np.argmax(np.random.multinomial(1, py, n), axis=1)\n",
    "    z = np.argmax([np.random.multinomial(1, p) for p in pz[x, y]], axis=1)\n",
    "    if partially_observed:\n",
    "        x = x.astype(object)\n",
    "        y = y.astype(object)\n",
    "        x[np.unique(np.random.choice(n, int(n/2)))] = None\n",
    "        if never_coobserved:\n",
    "            y[np.not_equal(x, None)] = None\n",
    "        else:\n",
    "            y[np.unique(np.random.choice(n, int(n/2)))] = None\n",
    "    return x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(random=False):\n",
    "    \"\"\" Initializes parameters for the EM algorithm\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    random : bool\n",
    "        If True, the parameters are set to random values (in range [0, 1] that sum to 1).\n",
    "        If False, all probabilities are 0.5 (binary variables).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    qx, qy, qx : numpy.ndarray\n",
    "        Initial parameters.\n",
    "    \"\"\"\n",
    "    if random:\n",
    "        qx = np.random.rand(2)\n",
    "        qy = np.random.rand(2)\n",
    "        qz = np.random.rand(2, 2, 2) \n",
    "    else:\n",
    "        qx = np.ones(2)\n",
    "        qy = np.ones(2)\n",
    "        qz = np.ones((2,2,2))\n",
    "    qx = qx / np.sum(qx)\n",
    "    qy = qy / np.sum(qy)\n",
    "    qz = qz / np.sum(qz, axis=2, keepdims=1)\n",
    "    return qx, qy, qz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tables(px, py, pz):\n",
    "    \"\"\" Prints probability tables in a nice way. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    px, py, pz : numpy.ndarray\n",
    "        Parameters, true or learnt.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    assert px.shape == (2,), f\"px = {px} should have shape (2,)\"\n",
    "    assert py.shape == (2,), f\"py = {py} should have shape (2,)\"\n",
    "    assert pz.shape == (2, 2, 2), f\"pz = {pz} should have shape (2, 2, 2)\"\n",
    "    print(f\"p(x) = {px}\")\n",
    "    print(f\"p(y) = {py}\")\n",
    "    print(f\"p(z|x=0, y=0) = {pz[0, 0]}\")\n",
    "    print(f\"p(z|x=0, y=1) = {pz[0, 1]}\")\n",
    "    print(f\"p(z|x=1, y=0) = {pz[1, 0]}\")\n",
    "    print(f\"p(z|x=1, y=1) = {pz[1, 1]}\")\n",
    "\n",
    "\n",
    "def print_marginals(px, py, pz):\n",
    "    \"\"\" Prints marginal probabilities of z given x or y, i.e. one variable is summed out.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    px, py, pz : numpy.ndarray\n",
    "        Parameters, true or learnt.\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    assert px.shape == (2,), f\"px = {px} should have shape (2,)\"\n",
    "    assert py.shape == (2,), f\"py = {py} should have shape (2,)\"\n",
    "    assert pz.shape == (2, 2, 2), f\"pz = {pz} should have shape (2, 2, 2)\"\n",
    "    print(f\"p(z|x=0) = {pz[0, 0] * py[0] + pz[0, 1] * py[1]}\")\n",
    "    print(f\"p(z|x=1) = {pz[1, 0] * py[0] + pz[1, 1] * py[1]}\")\n",
    "    print(f\"p(z|y=0) = {pz[0, 0] * px[0] + pz[1, 0] * px[1]}\")\n",
    "    print(f\"p(z|y=1) = {pz[0, 1] * px[0] + pz[1, 1] * px[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM algoritm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step(qx, qy, qz, xs, ys, zs):\n",
    "    \"\"\" Expectation step of EM.\n",
    "    Computes p(X|x, y, z), p(Y|x, y, z), and p(Z, X, Y|x, y, z), \n",
    "    normalizes them so each sums to 1, and adds to Mx, My, and Mz, respectively. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    qx, qy, qx : numpy.ndarray\n",
    "        ML parameter estimates of EM. Output of Maximization step in EM.\n",
    "    xs, ys, zs : numpy.ndarray\n",
    "        Generated data where a None in x or y is interpreted as missing data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Mx, My, Mz : numpy.ndarray\n",
    "        Sufficient statistics of the distribtuions of x,y, and z. Output of Expectation step in EM.\n",
    "    \"\"\"\n",
    "    Mx = np.zeros(2)\n",
    "    My = np.zeros(2)\n",
    "    Mz = np.zeros((2, 2, 2))\n",
    "    for x, y, z in zip(xs, ys, zs):\n",
    "        Q = np.zeros((2,2))\n",
    "        \n",
    "        if x is not None and y is not None:\n",
    "            Mx[x] += 1\n",
    "            My[y] += 1\n",
    "            Mz[x,y,z] += 1\n",
    "        else:\n",
    "            if x is None and y is not None:\n",
    "                Q[0,0] = qx[0] * (1-y) * qz[0,0,z]\n",
    "                Q[0,1] = qx[0] * y * qz[0,1,z]\n",
    "                Q[1,0] = qx[1] * (1-y) * qz[1,0,z]\n",
    "                Q[1,1] = qx[1] * y * qz[1,1,z]\n",
    "            elif x is not None and y is None:\n",
    "                Q[0,0] = (1-x) * qy[0] * qz[0,0,z]\n",
    "                Q[0,1] = (1-x) * qy[1] * qz[0,1,z]\n",
    "                Q[1,0] = x * qy[0] * qz[1,0,z]\n",
    "                Q[1,1] = x * qy[1] * qz[1,1,z]\n",
    "            else:\n",
    "                Q[0,0] = qx[0] * qy[0] * qz[0,0,z]\n",
    "                Q[0,1] = qx[0] * qy[1] * qz[0,1,z]\n",
    "                Q[1,0] = qx[1] * qy[0] * qz[1,0,z]\n",
    "                Q[1,1] = qx[1] * qy[1] * qz[1,1,z]\n",
    "                \n",
    "            Q /= np.sum(Q)\n",
    "            assert np.isclose(np.sum(Q), 1.0), f\"Q:{Q}, x:{x}, y:{y}, z:{z}\"\n",
    "            \n",
    "            if x is None:\n",
    "                Mx += np.sum(Q, axis=1)\n",
    "            else:\n",
    "                Mx[x] += 1\n",
    "            if y is None:\n",
    "                My += np.sum(Q, axis=0)\n",
    "            else:\n",
    "                My[y] += 1\n",
    "            \n",
    "            Mz[:,:,z] += Q\n",
    "            \n",
    "    return Mx, My, Mz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_step(Mx, My, Mz):\n",
    "    \"\"\" Maximization step of EM. Converts the sufficient statistics to ML parameter estimates.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Mx, My, Mz : numpy.ndarray\n",
    "        Sufficient statistics of the distribtuions of x,y, and z. Output of Expectation step in EM.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    qx, qy, qx : numpy.ndarray\n",
    "        ML parameter estimates of EM. Output of Maximization step in EM.\n",
    "    \"\"\"\n",
    "    qx = np.zeros(2)\n",
    "    qy = np.zeros(2)\n",
    "    qz = np.zeros((2,2,2))\n",
    "    \n",
    "    qx = Mx / np.sum(Mx)\n",
    "    qy = My / np.sum(My)\n",
    "    \n",
    "    qz[0,0,0] = Mz[0,0,0] / np.sum(Mz[0,0,:])\n",
    "    qz[0,0,1] = Mz[0,0,1] / np.sum(Mz[0,0,:])\n",
    "    \n",
    "    qz[0,1,0] = Mz[0,1,0] / np.sum(Mz[0,1,:])\n",
    "    qz[0,1,1] = Mz[0,1,1] / np.sum(Mz[0,1,:])\n",
    "    \n",
    "    qz[1,0,0] = Mz[1,0,0] / np.sum(Mz[1,0,:])\n",
    "    qz[1,0,1] = Mz[1,0,1] / np.sum(Mz[1,0,:])\n",
    "    \n",
    "    qz[1,1,0] = Mz[1,1,0] / np.sum(Mz[1,1,:])\n",
    "    qz[1,1,1] = Mz[1,1,1] / np.sum(Mz[1,1,:])\n",
    "    \n",
    "    return qx, qy, qz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM with no missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having implemented the E-step and the M-step for complete data the results shown below were obtained with the numpy random seed set to 1337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learnt parameters\n",
      "-----------------\n",
      "p(x) = [0.568 0.432]\n",
      "p(y) = [0.294 0.706]\n",
      "p(z|x=0, y=0) = [0.24137931 0.75862069]\n",
      "p(z|x=0, y=1) = [0.63451777 0.36548223]\n",
      "p(z|x=1, y=0) = [0.88333333 0.11666667]\n",
      "p(z|x=1, y=1) = [0.1025641 0.8974359]\n",
      "\n",
      "True parameters\n",
      "---------------\n",
      "p(x) = [0.6 0.4]\n",
      "p(y) = [0.3 0.7]\n",
      "p(z|x=0, y=0) = [0.2 0.8]\n",
      "p(z|x=0, y=1) = [0.7 0.3]\n",
      "p(z|x=1, y=0) = [0.9 0.1]\n",
      "p(z|x=1, y=1) = [0.1 0.9]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "px = np.array([0.6, 0.4])\n",
    "py = np.array([0.3, 0.7])\n",
    "pz = np.array([[[0.2, 0.8], [0.7, 0.3]], [[0.9, 0.1], [0.1, 0.9]]])  # p(z|x, y) = pz[x, y, z]\n",
    "n_data = 500\n",
    "x, y, z = generate_data(px, py, pz, n_data)\n",
    "n_iter = 10\n",
    "qx, qy, qz = expectation_maximization(x, y, z, e_step, m_step, n_iter)\n",
    "\n",
    "print(\"Learnt parameters\")\n",
    "print(\"-----------------\")\n",
    "print_tables(qx, qy, qz)\n",
    "print()\n",
    "print(\"True parameters\")\n",
    "print(\"---------------\")\n",
    "print_tables(px, py, pz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the print-out above, in the case of complete data, the EM algorithm accurately estimates the parameters of the Bayesian network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the random seed is set to 2018, the results shown below were obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learnt parameters\n",
      "-----------------\n",
      "p(x) = [0.586 0.414]\n",
      "p(y) = [0.316 0.684]\n",
      "p(z|x=0, y=0) = [0.20212766 0.79787234]\n",
      "p(z|x=0, y=1) = [0.70351759 0.29648241]\n",
      "p(z|x=1, y=0) = [0.890625 0.109375]\n",
      "p(z|x=1, y=1) = [0.09090909 0.90909091]\n",
      "\n",
      "True parameters\n",
      "---------------\n",
      "p(x) = [0.6 0.4]\n",
      "p(y) = [0.3 0.7]\n",
      "p(z|x=0, y=0) = [0.2 0.8]\n",
      "p(z|x=0, y=1) = [0.7 0.3]\n",
      "p(z|x=1, y=0) = [0.9 0.1]\n",
      "p(z|x=1, y=1) = [0.1 0.9]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2018)\n",
    "px = np.array([0.6, 0.4])\n",
    "py = np.array([0.3, 0.7])\n",
    "pz = np.array([[[0.2, 0.8], [0.7, 0.3]], [[0.9, 0.1], [0.1, 0.9]]])  # p(z|x, y) = pz[x, y, z]\n",
    "n_data = 500\n",
    "x, y, z = generate_data(px, py, pz, n_data)\n",
    "n_iter = 10\n",
    "qx, qy, qz = expectation_maximization(x, y, z, e_step, m_step, n_iter)\n",
    "\n",
    "print(\"Learnt parameters\")\n",
    "print(\"-----------------\")\n",
    "print_tables(qx, qy, qz)\n",
    "print()\n",
    "print(\"True parameters\")\n",
    "print(\"---------------\")\n",
    "print_tables(px, py, pz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogously to the previous random seed, in the case of the random seed of 2018, the network parameters are accurately estimated with the EM algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM with missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When estimating the parameters of the network with the EM algorithm for partially observed data, the results shown below were obtained with the random seed set to 1337. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learnt parameters\n",
      "-----------------\n",
      "p(x) = [0.58130798 0.41869202]\n",
      "p(y) = [0.28239202 0.71760798]\n",
      "p(z|x=0, y=0) = [0.28220124 0.71779876]\n",
      "p(z|x=0, y=1) = [0.60052118 0.39947882]\n",
      "p(z|x=1, y=0) = [0.92109464 0.07890536]\n",
      "p(z|x=1, y=1) = [0.09122205 0.90877795]\n",
      "\n",
      "True parameters\n",
      "---------------\n",
      "p(x) = [0.6 0.4]\n",
      "p(y) = [0.3 0.7]\n",
      "p(z|x=0, y=0) = [0.2 0.8]\n",
      "p(z|x=0, y=1) = [0.7 0.3]\n",
      "p(z|x=1, y=0) = [0.9 0.1]\n",
      "p(z|x=1, y=1) = [0.1 0.9]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "px = np.array([0.6, 0.4])\n",
    "py = np.array([0.3, 0.7])\n",
    "pz = np.array([[[0.2, 0.8], [0.7, 0.3]], [[0.9, 0.1], [0.1, 0.9]]])\n",
    "n_data = 500\n",
    "x, y, z = generate_data(px, py, pz, n_data, partially_observed=True, never_coobserved=False)\n",
    "n_iter = 10\n",
    "qx, qy, qz = expectation_maximization(x, y, z, e_step, m_step, n_iter)\n",
    "\n",
    "print(\"Learnt parameters\")\n",
    "print(\"-----------------\")\n",
    "print_tables(qx, qy, qz)\n",
    "print()\n",
    "print(\"True parameters\")\n",
    "print(\"---------------\")\n",
    "print_tables(px, py, pz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the seed was set to 2018, the results shown below were obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learnt parameters\n",
      "-----------------\n",
      "p(x) = [0.61579605 0.38420395]\n",
      "p(y) = [0.31324231 0.68675769]\n",
      "p(z|x=0, y=0) = [0.24857963 0.75142037]\n",
      "p(z|x=0, y=1) = [0.69335717 0.30664283]\n",
      "p(z|x=1, y=0) = [0.87966812 0.12033188]\n",
      "p(z|x=1, y=1) = [0.06643502 0.93356498]\n",
      "\n",
      "True parameters\n",
      "---------------\n",
      "p(x) = [0.6 0.4]\n",
      "p(y) = [0.3 0.7]\n",
      "p(z|x=0, y=0) = [0.2 0.8]\n",
      "p(z|x=0, y=1) = [0.7 0.3]\n",
      "p(z|x=1, y=0) = [0.9 0.1]\n",
      "p(z|x=1, y=1) = [0.1 0.9]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2018)\n",
    "px = np.array([0.6, 0.4])\n",
    "py = np.array([0.3, 0.7])\n",
    "pz = np.array([[[0.2, 0.8], [0.7, 0.3]], [[0.9, 0.1], [0.1, 0.9]]])\n",
    "n_data = 500\n",
    "x, y, z = generate_data(px, py, pz, n_data, partially_observed=True, never_coobserved=False)\n",
    "n_iter = 10\n",
    "qx, qy, qz = expectation_maximization(x, y, z, e_step, m_step, n_iter)\n",
    "\n",
    "print(\"Learnt parameters\")\n",
    "print(\"-----------------\")\n",
    "print_tables(qx, qy, qz)\n",
    "print()\n",
    "print(\"True parameters\")\n",
    "print(\"---------------\")\n",
    "print_tables(px, py, pz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the random seed of 2018, the network parameters are again accurately estimated with the EM algorithm for partially observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM with missing data with modified generating distributions and fewer instances in data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, experiments were run with modified generating distributions (px, py, pz), then experiments were run on data with only a few instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first experiment, the generating distributions were modified to have different parameters. The results shown below were obtained from learning the parameters of the Bayesian network with the EM algorithm for partially observed data, with the random seed set to 1337."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learnt parameters\n",
      "-----------------\n",
      "p(x) = [0.51679318 0.48320682]\n",
      "p(y) = [0.28320376 0.71679624]\n",
      "p(z|x=0, y=0) = [0.16650611 0.83349389]\n",
      "p(z|x=0, y=1) = [0.39762683 0.60237317]\n",
      "p(z|x=1, y=0) = [0.78143991 0.21856009]\n",
      "p(z|x=1, y=1) = [0.17100803 0.82899197]\n",
      "\n",
      "True parameters\n",
      "---------------\n",
      "p(x) = [0.5 0.5]\n",
      "p(y) = [0.3 0.7]\n",
      "p(z|x=0, y=0) = [0.2 0.8]\n",
      "p(z|x=0, y=1) = [0.4 0.6]\n",
      "p(z|x=1, y=0) = [0.85 0.15]\n",
      "p(z|x=1, y=1) = [0.1 0.9]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "px = np.array([0.5, 0.5])\n",
    "py = np.array([0.3, 0.7])\n",
    "pz = np.array([[[0.2, 0.8], [0.4, 0.6]], [[0.85, 0.15], [0.1, 0.9]]])\n",
    "n_data = 500\n",
    "x, y, z = generate_data(px, py, pz, n_data, partially_observed=True, never_coobserved=False)\n",
    "n_iter = 10\n",
    "qx, qy, qz = expectation_maximization(x, y, z, e_step, m_step, n_iter)\n",
    "\n",
    "print(\"Learnt parameters\")\n",
    "print(\"-----------------\")\n",
    "print_tables(qx, qy, qz)\n",
    "print()\n",
    "print(\"True parameters\")\n",
    "print(\"---------------\")\n",
    "print_tables(px, py, pz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the print-out results, the parameters learned by the EM algorithm are accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second experiment, the generating distributions were modified with the aim to evaluate the performance of the EM algorithm when some probabilities are set to zero. With the random seed set to 1337, the results shown below were obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learnt parameters\n",
      "-----------------\n",
      "p(x) = [4.79278354e-05 9.99952072e-01]\n",
      "p(y) = [0.52123828 0.47876172]\n",
      "p(z|x=0, y=0) = [0.9387989 0.0612011]\n",
      "p(z|x=0, y=1) = [6.49002116e-04 9.99350998e-01]\n",
      "p(z|x=1, y=0) = [0.91702921 0.08297079]\n",
      "p(z|x=1, y=1) = [1.81026233e-05 9.99981897e-01]\n",
      "\n",
      "True parameters\n",
      "---------------\n",
      "p(x) = [0. 1.]\n",
      "p(y) = [0.5 0.5]\n",
      "p(z|x=0, y=0) = [0.2 0.8]\n",
      "p(z|x=0, y=1) = [0.7 0.3]\n",
      "p(z|x=1, y=0) = [0.9 0.1]\n",
      "p(z|x=1, y=1) = [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "px = np.array([0.0, 1.0])\n",
    "py = np.array([0.5, 0.5])\n",
    "pz = np.array([[[0.2, 0.8], [0.7, 0.3]], [[0.9, 0.1], [0.0, 1.0]]])\n",
    "n_data = 500\n",
    "x, y, z = generate_data(px, py, pz, n_data, partially_observed=True, never_coobserved=False)\n",
    "n_iter = 10\n",
    "qx, qy, qz = expectation_maximization(x, y, z, e_step, m_step, n_iter)\n",
    "\n",
    "print(\"Learnt parameters\")\n",
    "print(\"-----------------\")\n",
    "print_tables(qx, qy, qz)\n",
    "print()\n",
    "print(\"True parameters\")\n",
    "print(\"---------------\")\n",
    "print_tables(px, py, pz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the results above, the EM algorithm did not succeed in accurately learning all of the parameters. The generating distributions px and py for node X and Y in the network are accurately estimated. However, the estimations for the generating distribution pz for node Z in the network have shortcomings. Firstly, the learned distributions p(z|x=1, y=0) and p(z|x=1, y=1) are accurate when compared to their true counterparts. Nevertheless, the learned distributions p(z|x=0, y=0) and p(z|x=0, y=1) are rather inaccurate of their true counterparts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the third experiment, the generating distribtuions were reset to their original values and the number of instances in the partially observed data set were reduced to n_data = 100. With the the random seed set to 1337, the results shown below were obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learnt parameters\n",
      "-----------------\n",
      "p(x) = [0.64843219 0.35156781]\n",
      "p(y) = [0.25703223 0.74296777]\n",
      "p(z|x=0, y=0) = [0.23808872 0.76191128]\n",
      "p(z|x=0, y=1) = [0.49356376 0.50643624]\n",
      "p(z|x=1, y=0) = [0.79389173 0.20610827]\n",
      "p(z|x=1, y=1) = [0.19638459 0.80361541]\n",
      "\n",
      "True parameters\n",
      "---------------\n",
      "p(x) = [0.6 0.4]\n",
      "p(y) = [0.3 0.7]\n",
      "p(z|x=0, y=0) = [0.2 0.8]\n",
      "p(z|x=0, y=1) = [0.7 0.3]\n",
      "p(z|x=1, y=0) = [0.9 0.1]\n",
      "p(z|x=1, y=1) = [0.1 0.9]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "px = np.array([0.6, 0.4])\n",
    "py = np.array([0.3, 0.7])\n",
    "pz = np.array([[[0.2, 0.8], [0.7, 0.3]], [[0.9, 0.1], [0.1, 0.9]]])  # p(z|x, y) = pz[x, y, z]\n",
    "n_data = 100\n",
    "x, y, z = generate_data(px, py, pz, n_data, partially_observed=True, never_coobserved=False)\n",
    "n_iter = 10\n",
    "qx, qy, qz = expectation_maximization(x, y, z, e_step, m_step, n_iter)\n",
    "\n",
    "print(\"Learnt parameters\")\n",
    "print(\"-----------------\")\n",
    "print_tables(qx, qy, qz)\n",
    "print()\n",
    "print(\"True parameters\")\n",
    "print(\"---------------\")\n",
    "print_tables(px, py, pz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, reducing the number of instances in the partially observed data set from the original 500 to 100 had a slight detrimental effect on the accuracy of the parameters learned by the EM algorithm. The modest degradation of the accuracy manifests itself in all of the learned parameters, the most inaccurate of which is the generating distribution p(z|x=0, y=1). Nevertheless, the parameters estimations are still generally accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the fourth and last experiment, the number of instances in the data set was further reduced from 100 to 10. With the random seed set to 1337, the results shown below were obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learnt parameters\n",
      "-----------------\n",
      "p(x) = [0.81798999 0.18201001]\n",
      "p(y) = [0.22019805 0.77980195]\n",
      "p(z|x=0, y=0) = [0. 1.]\n",
      "p(z|x=0, y=1) = [0.47519945 0.52480055]\n",
      "p(z|x=1, y=0) = [0. 1.]\n",
      "p(z|x=1, y=1) = [1.28623669e-10 1.00000000e+00]\n",
      "\n",
      "True parameters\n",
      "---------------\n",
      "p(x) = [0.6 0.4]\n",
      "p(y) = [0.3 0.7]\n",
      "p(z|x=0, y=0) = [0.2 0.8]\n",
      "p(z|x=0, y=1) = [0.7 0.3]\n",
      "p(z|x=1, y=0) = [0.9 0.1]\n",
      "p(z|x=1, y=1) = [0.1 0.9]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "px = np.array([0.6, 0.4])\n",
    "py = np.array([0.3, 0.7])\n",
    "pz = np.array([[[0.2, 0.8], [0.7, 0.3]], [[0.9, 0.1], [0.1, 0.9]]])  # p(z|x, y) = pz[x, y, z]\n",
    "n_data = 10\n",
    "x, y, z = generate_data(px, py, pz, n_data, partially_observed=True, never_coobserved=False)\n",
    "n_iter = 50\n",
    "qx, qy, qz = expectation_maximization(x, y, z, e_step, m_step, n_iter)\n",
    "\n",
    "print(\"Learnt parameters\")\n",
    "print(\"-----------------\")\n",
    "print_tables(qx, qy, qz)\n",
    "print()\n",
    "print(\"True parameters\")\n",
    "print(\"---------------\")\n",
    "print_tables(px, py, pz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the results above, the EM algorithm did not succeed at learning the parameters with the exception of the generating distribution p(y), which is approximately close to its true counterpart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, the EM algorithm accurately estimates the parameters of the generating distributions of the Bayesian network when the parameters of the true generating distributions are modified. However, reducing the number of instances in the partially observed data set deteriorates the accuracy of the estimates. For a mere 10 data points, the algorithm fails to learn accurate parameter estimates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM with missing data that are never co-observed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the variables x and y (the parents of z) are never co-observed such that y is missing if and only if x is not missing, i.e. all data points are either (None, y, z) or (x, None, z)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the original generating distributions the results shown below were obtained for learning the parameters of the Bayesian network with the EM algorithm for partially observed, and never co-observed data, with the random seed set to 1337."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learnt parameters\n",
      "-----------------\n",
      "p(x) = [0.57023911 0.42976089]\n",
      "p(y) = [0.28274479 0.71725521]\n",
      "p(z|x=0, y=0) = [0.52164892 0.47835108]\n",
      "p(z|x=0, y=1) = [0.51070207 0.48929793]\n",
      "p(z|x=1, y=0) = [0.28882444 0.71117556]\n",
      "p(z|x=1, y=1) = [0.33063173 0.66936827]\n",
      "\n",
      "True parameters\n",
      "---------------\n",
      "p(x) = [0.6 0.4]\n",
      "p(y) = [0.3 0.7]\n",
      "p(z|x=0, y=0) = [0.2 0.8]\n",
      "p(z|x=0, y=1) = [0.7 0.3]\n",
      "p(z|x=1, y=0) = [0.9 0.1]\n",
      "p(z|x=1, y=1) = [0.1 0.9]\n",
      "\n",
      "Learnt marginals\n",
      "----------------\n",
      "p(z|x=0) = [0.51379724 0.48620276]\n",
      "p(z|x=1) = [0.31881093 0.68118907]\n",
      "p(z|y=0) = [0.42159006 0.57840994]\n",
      "p(z|y=1) = [0.43331488 0.56668512]\n",
      "\n",
      "True marginals\n",
      "--------------\n",
      "p(z|x=0) = [0.55 0.45]\n",
      "p(z|x=1) = [0.34 0.66]\n",
      "p(z|y=0) = [0.48 0.52]\n",
      "p(z|y=1) = [0.46 0.54]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "px = np.array([0.6, 0.4])\n",
    "py = np.array([0.3, 0.7])\n",
    "pz = np.array([[[0.2, 0.8], [0.7, 0.3]], [[0.9, 0.1], [0.1, 0.9]]])  # p(z|x, y) = pz[x, y, z]\n",
    "n_data = 500\n",
    "x, y, z = generate_data(px, py, pz, n_data, partially_observed=True, never_coobserved=True)\n",
    "n_iter = 10\n",
    "qx, qy, qz = expectation_maximization(x, y, z, e_step, m_step, n_iter)\n",
    "\n",
    "print(\"Learnt parameters\")\n",
    "print(\"-----------------\")\n",
    "print_tables(qx, qy, qz)\n",
    "print()\n",
    "print(\"True parameters\")\n",
    "print(\"---------------\")\n",
    "print_tables(px, py, pz)\n",
    "\n",
    "print()\n",
    "print(\"Learnt marginals\")\n",
    "print(\"----------------\")\n",
    "print_marginals(qx, qy, qz)\n",
    "print()\n",
    "print(\"True marginals\")\n",
    "print(\"--------------\")\n",
    "print_marginals(px, py, pz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the results above, the EM algorithm did not succeed at estimating the generating distribution parameters with the exception of p(x) and p(y), which are approximately close to their true counterparts. Since the nodes x and y are the parents of z, always missing either one of the parent values significantly worsens the estimation of the conditional generating distribution p(z|x, y) of node z. As a result, the parameter estimates of the distribution $p(z|x, y)$ are nowhere near their true values. The notably worse parameter estimates are due to the fact the data observability model is not Missing At Random (MAR) anymore since the values of the observation variables of x and y are not independent of the missing data points. What is more, actually, a non-missing x causes y to be missing and a missing x causes y to be non-missing. For the EM algorithm to work, the data observability model has to be MAR, or be converted to MAR via adding more nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Add code to visualize the expected complete data log-likelihood and how it is maximized until a local maximum during the iterations of the EM.\n",
    "2. Add more nodes to convert the \"never co-observed x and y\" case to MAR. Extend EM algorithm to accomodate the new node/nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. DD2420 Probabilistic Graphical Models Course Lecture Notes and Laboratory Guide at the KTH Royal Institute of Technology (some of the code was based on notes and lab documents)\n",
    "2. Probabilistic Graphical Models: Principles and Techniques by Daphne Koller and Nir Friedman (some of the ideas for the code were based on their outstanding book)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "em",
   "language": "python",
   "name": "em"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
